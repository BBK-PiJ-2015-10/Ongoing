package com.datax.play.spark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql._

case class Auction(auctionId: String, bid:Float, bidtime: Float, 
  bidder:String, bidderrate: Int, opebid:Float, price: Float,
  item:String, daystolive: Int
)


object SQLExample {
  
  def main(args:Array[String]) {
    
    val conf = new SparkConf().setAppName("SparkSQLUseCase").setMaster("local[*]")
    
    val sc = new SparkContext(conf)
    
    //SQL entry point for working with structured data
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    
    //This is use to implicitily convert an RDD to a DataFrame
    import sqlContext.implicits._
    
    import sqlContext._
   
    //Load data into an RDD
    val ebayText = sc.textFile("auctions/*.csv")
    
    //Create an RDD of Auctions objects
    
    val ebay = ebayText.map(_.split(",")).map(p=>Auction(
        
    ))
    
    
    println("Tontolone")
    
  }
  
  
}