package com.datax.play.spark

import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext

object counter {
  
    val conf = new SparkConf().setAppName("counter").setMaster("local[*]")
    
    val sc= new SparkContext(conf)
    
    val text = sc.textFile("input/*.txt")  
  
  def main (args: Array[String]): Unit = {
    
      val t1 = "Berlin"
      val t2 = "Germany"
      countTuples(t1,t2)
      
      
     //val test = List("yasser","alejandro","alberto","palacios","otero","palacios","cortez","palacios","tonto","palacios","cortez")
     //val t1 ="palacios"
     //val t2 ="cortez"
     
     //println(countTargets(t1,t2,test))
     
    
      //text.flatMap{line=>line.split(" ")}.map{word=>(word,1)}.reduceByKey(_+_).saveAsTextFile("output.txt")
      
      //println(text.flatMap{line=>line.split(" ")}.filter(x=>x.equals("Berlin")).count())
          
    
    //This expression creates a tuple of the words in the document and its counts
    /*
    
    text.flatMap{line=>line.split(" ")}
    .map{word=>(word,1)}
    .reduceByKey(_+_).saveAsTextFile("output.txt")
    
    */
    
    //println("Work is completed")
    
    
    
  }
  
  def countTuples (first: String, second: String) : Double = {
    val array = text.flatMap{line=>line.split(" ")}
    val list = array.collect().toList
    val deno = array.filter(x=>x.equals(first)).count().toDouble
    val nume = countTargets(first,second,list).toDouble
    return nume/deno
  }
  
  
  def countTarget(first: String) : Long = {  
    return text.flatMap{line=>line.split(" ")}.filter(x=>x.equals(first)).count()
    
    //text.flatMap{line=>line.split(" ")}.foreach(x=>println(x))
  
    
  }
  
  def countTargets(target1 : String, target2 : String, list :List[String]) : Long = {
    var count=0
    for ( i <- 0 to list.size-2){
      val t1 = list.apply(i)
      val t2 = list.apply(i+1)
      if (t1.equals(target1) && t2.equals(target2)){
        count +=1
      }
    }
    return count
  }
  
  
}