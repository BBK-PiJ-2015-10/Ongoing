package com.datax.play.spark

import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext

object counter {
  
    val conf = new SparkConf().setAppName("counter").setMaster("local[*]")
    
    val sc= new SparkContext(conf)
    
    val text = sc.textFile("input/*.txt")  
  
  def main (args: Array[String]): Unit = {
    
      //println(countTuples("Berlin","Frankfurt")) 
      
      nextWords("Berlin").foreach(x=>println(x))
      
      println("What happened")
    
  }
    
  def nextWords (first: String) : List[String] = {
    var result = List("ale")
    val list = text.flatMap{line=>line.split(" ")}.collect().toList 
    for (i <- 0 to list.size-2){
      if (list.apply(i).equals(first)){
        result :+ list.apply(i+1)
      }
    }
    println(result.size)
    return result
  }
  
  
  def countTuples (first: String, second: String) : Double = {
    val array = text.flatMap{line=>line.split(" ")}
    val list = array.collect().toList
    val deno = array.filter(x=>x.equals(first)).count().toDouble
    val nume = countTargets(first,second,list).toDouble
    println(first +" was found : " +deno +" times")
    println(second +" was found : " +nume +" times")
    return nume/deno
  }
  
  
  def countTarget(first: String) : Long = {  
    return text.flatMap{line=>line.split(" ")}.filter(x=>x.equals(first)).count()
  }
  
  def countTargets(target1 : String, target2 : String, list :List[String]) : Long = {
    var count=0
    for ( i <- 0 to list.size-2){
      if (list.apply(i).equals(target1) && list.apply(i+1).equals(target2)){
        count +=1
      }
    }
    return count
  }
  
  
  def createTuples() : Unit = {
    text.flatMap{line=>line.split(" ")}
    .map{word=>(word,1)}
    .reduceByKey(_+_).saveAsTextFile("output.txt")
  }
  
  
}